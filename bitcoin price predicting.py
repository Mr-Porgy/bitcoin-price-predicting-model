# -*- coding: utf-8 -*-
"""BDML_final_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ohh4hS5vRqrjSy6xCEgyy62TqqCkQHxY
"""



"""# **Big Data and Machine Learning Final Project**


## **Jie Zhou**

##    Forecasting Price change of Cryptocurrencies
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import datetime as dt

import statsmodels.api as sm
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller

import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns; sns.set_style("whitegrid")
from plotly import tools
import plotly.offline as py
py.init_notebook_mode(connected=True)
import plotly.graph_objs as go

from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor
from sklearn.linear_model import BayesianRidge, ElasticNetCV
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

import numpy as np
import pandas as pd
import datetime as dt

import statsmodels.api as sm
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller

import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns; sns.set_style("whitegrid")
from plotly import tools
import plotly.offline as py
py.init_notebook_mode(connected=True)
import plotly.graph_objs as go

from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor
from sklearn.linear_model import BayesianRidge, ElasticNetCV
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

import pandas as pd
pd.core.common.is_list_like = pd.api.types.is_list_like
from pandas_datareader import data as pdr
import matplotlib.pyplot as plt 
import fix_yahoo_finance as yf

yf.pdr_override()
Df = pdr.get_data_yahoo('BTC-USD','2017-01-01','2019-05-01')

Df.to_csv('BTC2016TONOW.CSV')
#Df = pdr.get_data_yahoo('GLD','2012-01-01','2018-08-10')

df_BTC=Df.copy()
#alternately, you can try : Df=yf.download('GLD',start='2008-01-01',end='2018-01-01')
Df=Df[['Close']] 
Df= Df.dropna() 
Df.Close.plot(figsize=(10,5)) 
plt.ylabel("BTC Prices")
plt.show()

# yf.pdr_override()
# Df = pdr.get_data_yahoo('SPY','2015-01-01','2018-08-10')

# #Df = pdr.get_data_yahoo('GLD','2012-01-01','2018-08-10')

# df_spy=Df.copy()
# #alternately, you can try : Df=yf.download('GLD',start='2008-01-01',end='2018-01-01')
# Df=Df[['Close']] 
# Df= Df.dropna() 
# Df.Close.plot(figsize=(10,5)) 
# plt.ylabel("SPY ETF Prices")
# plt.show()

#Df.to_csv('BTC2017TO2019')

# print(df_spy.head())

# print(df_spy.shape)

# Test by SPY

# df_spy['close_After_20']=df_spy['Close'].shift(-30)
# df_spy['change_in_20']=df_spy['close_After_20']-df_spy['Close']
# df_spy['change_in_20']=df_spy['change_in_20']/abs(df_spy['change_in_20'])



# df_spy['ma3'] = df_spy['Close'].shift(1).rolling(window=3).mean() 
# df_spy['ma9']= df_spy['Close'].shift(1).rolling(window=9).mean() 
# df_spy= df_spy.dropna() 


# X_spy = df_spy.drop(['close_After_20','change_in_20'], axis=1)
# y_spy = df_spy.dropna()['close_After_20']
# y2_spy = df_spy.dropna()['change_in_20']

df_BTC['close_After_20']=df_BTC['Close'].shift(-20)
df_BTC['change_in_20']=df_BTC['close_After_20']-df_BTC['Close'].shift(-19)

#df_BTC['change_in_20']=df_BTC['close_After_20']-df_BTC['Close']
df_BTC['change_in_20']=df_BTC['change_in_20']/abs(df_BTC['change_in_20'])



df_BTC['ma3'] = df_BTC['Close'].shift(1).rolling(window=3).mean() 
df_BTC['ma9']= df_BTC['Close'].shift(1).rolling(window=9).mean() 
df_BTC= df_BTC.dropna() 

X_BTC=df_BTC
# X_BTC = df_BTC.drop(['close_After_20','change_in_20'], axis=1)
# y_BTC = df_BTC.dropna()['close_After_20']
# y2_BTC = df_BTC.dropna()['change_in_20']

# y2_BTC.head()

X_BTC.head()

X_BTC.iloc[20]



# X.append(xiv_changes[i:i+context.lookback]) # Store prior volume changes
#         Y.append(xiv_changes[i+context.lookback])

new_x_BTC=[]
new_x2_BTC=[]
new_y_BTC=[]
new_y2_BTC=[]

for i in range(0,len(X_BTC)-20):
  new_x_BTC.append(X_BTC.iloc[0+i:20+i]['Close'].tolist())
  new_x2_BTC.append(np.diff(X_BTC.iloc[0+i:20+i]['Close'].tolist()))
  new_y_BTC.append(X_BTC.iloc[i+20]['Close'])
  new_y2_BTC.append(X_BTC.iloc[i]['change_in_20'].tolist())
# new_y_BTC=y_BTC[:len(X_BTC)-20]
# new_y2_BTC=y2_BTC[:len(X_BTC)-20]



print(new_x_BTC[4])
print(new_y_BTC[3])
print(new_y2_BTC[0])

print(len(new_x_BTC))
print(len(new_y_BTC))
print(len(new_y2_BTC))

# abc=X_BTC.iloc[0:20]['Close'].tolist()
# abc
print(len(new_x2_BTC))
#new_y_BTC.head()
#y2_time=X_BTC.iloc[19:]['change_in_20'].shape
X_BTC.tail()


X_BTC['close_After_20'].iloc[:770].shape
X_BTC['close_After_20'].iloc[:770].tail()

y_time=X_BTC['close_After_20'].iloc[:770]

print(y_time.tail(10))

#print(y.tail(10))

df_x_new = pd.DataFrame(new_x_BTC)
df_x2_new = pd.DataFrame(new_x2_BTC)

df_y_new=pd.DataFrame(new_y_BTC)
df_y2_new=pd.DataFrame(new_y2_BTC)
print(df_y_new.tail(30))
print(df_y2_new.tail(30))
df_x_new.tail(30)

# print(y_BTC.head())
# print(y2_BTC.head())

# print(y_BTC.shape)
# print(y2_BTC.shape)

# print(X_BTC.shape)

# print(df_BTC.head())

# print(df_BTC[20:25]['Close'])

# X_train, X_test, y_train, y_test = train_test_split(X_BTC, y2_BTC, 
#                             test_size=0.3, 
#                             random_state=1)

# new_x_test=[]

# for i in range(0,len(X_test)-20):
#   new_x_test.append(X_test.iloc[0+i:19+i])


# new_y2_BTC=y2_BTC.iloc[:297]





# len(new_x_test)
# len(y_train[:len(y_train)-20])

# len(y2_BTC)
# len(new_x_BTC)
# len(new_y2_BTC)

# print(X_train.shape)
# print(y_train.shape)

# print(len(new_x_train))

# X_train[:2]
# new_x_train[:2]
# y_train[:1]

X=df_x_new
X2=df_x2_new
y=df_y_new
y2=df_y2_new

y2.shape

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

import warnings
warnings.filterwarnings("ignore")

from sklearn.linear_model import Perceptron, LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import LinearSVC

from sklearn.model_selection import cross_val_score


X_train, X_test, y_train, y_test = train_test_split(X, y2, 
                            test_size=0.3, 
                            random_state=1)

names = [
         "LogisticRegression",
         "Linear SVM",
         "Decision Tree", 
         "Random Forest", 
         "RBF SVM",          
         "Neural Net", 
         "Naive Bayes",  
         "Nearest Neighbors"] 

classifiers = [

    LogisticRegression(),
    LinearSVC(C=1, dual = False), 
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=100, max_features=1),
    SVC(kernel='rbf',gamma=2, C=1),
    MLPClassifier(hidden_layer_sizes=(100,50), alpha=1),
    GaussianNB(),
    KNeighborsClassifier(5)]


for name, clf in zip(names, classifiers):
    clf.fit(X_train, y_train)
    score = clf.score(X_test, y_test)
    print("Algo: {0:<20s} and Score: {1:0.4f}".format(name, score))

y_test.head()

# from sklearn.model_selection import train_test_split
# from sklearn.preprocessing import StandardScaler

# #import warnings
# #warnings.filterwarnings("ignore")

# from sklearn.linear_model import Perceptron, LogisticRegression
# from sklearn.svm import SVC
# from sklearn.tree import DecisionTreeClassifier
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.neural_network import MLPClassifier
# from sklearn.naive_bayes import GaussianNB
# from sklearn.neighbors import KNeighborsClassifier


# from sklearn.model_selection import cross_val_score

# def get_acc(y_test,y_pred,tol):
#   num_acc=0
#   for i in range(0,len(y_pred)):
#     if(y_pred[i] > y_test[i]-tol and y_pred[i] < y_test[i]+tol):
#       num_acc+=1
#   return num_acc/len(y_pred)
  
# X_train, X_test, y_train, y_test = train_test_split(X, y_time, 
#                             test_size=0.3, 
#                             random_state=0)

# Regressor = {
#         'Random Forest Regressor': RandomForestRegressor(n_estimators=200),
#         'Gradient Boosting Regressor': GradientBoostingRegressor(n_estimators=500),
#         'ExtraTrees Regressor': ExtraTreesRegressor(n_estimators=500, min_samples_split=5),
#         'Bayesian Ridge': BayesianRidge(),
#         'Elastic Net CV': ElasticNetCV()
#     }


# for name, clf in Regressor.items():
#     print(name)
#     clf.fit(X_train, y_train)

#     print('acc',clf.score(X_test, y_test))  
#     #print('new_acc',get_acc(y_test,clf.predict(X_test),10))




    
# #         print(f'R2: {r2_score(y_test, clf.predict(X_test)):.2f}')
# #         print(f'MAE: {mean_absolute_error(y_test, clf.predict(X_test)):.2f}')
# #         print(f'MSE: {mean_squared_error(y_test, clf.predict(X_test)):.2f}')

# X_train, X_test, y_train, y_test = train_test_split(X, y_time, test_size=0.2, random_state=0)

# random_forest=RandomForestRegressor(n_estimators=200).fit(X_train, y_train)
# predicted_price = random_forest.predict(X_test) 

# predicted_price = pd.DataFrame(predicted_price,index=y_test.index,columns = ['Price'])  
# predicted_price.plot(figsize=(10,5))  
# y_test.plot()  
# plt.legend(['Actual_price'])  
# plt.ylabel("BTC Price")  
# plt.show()


# y_test.head()

"""# **Using Cross-Validation along with PCA and StandardScaler to Predict**"""

from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.decomposition import PCA
import warnings
warnings.filterwarnings("ignore")

X_train, X_test, y_train, y_test = train_test_split(X, y2, 
                            test_size=0.3, 
                            random_state=1)

# add PCA in Pipeline to redo the Cross-Validation
from sklearn.model_selection import cross_val_score

X_train, X_test, y_train, y_test = train_test_split(X, y2, 
                            test_size=0.3, 
                            random_state=1)

for name, clf in zip(names, classifiers):
    # add pipeline to convert all data to StandardScale does it help?
    pipe = Pipeline( [ ('scl', StandardScaler()),
                   ('pca', PCA(n_components = 5)),
                   ('clf', clf)])
    pipe.fit(X_train, y_train)
    
    scores = cross_val_score(pipe, X_test, y_test, cv=10)
    print("Algo: {0:<20s} and Score: {1:0.4f}".format(name, np.mean(scores)))



"""# **Using GridSearch to Optimize**"""

X_train, X_test, y_train, y_test = train_test_split(X, y2, 
                            test_size=0.3, 
                            random_state=1)

# Using GridSearch to Optimize SVC
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC

pipe_svc = Pipeline([('scl', StandardScaler()), 
                     ('pca', PCA(n_components = 5)),
                     ('clf', SVC(random_state=1))])

param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0]

param_grid = [{'clf__C': param_range,
               'clf__kernel': ['linear']},
              {'clf__C': param_range,
               'clf__gamma': param_range,
               'clf__kernel': ['rbf']}]

gs = GridSearchCV(estimator = pipe_svc,
                 param_grid = param_grid,
                 scoring = 'accuracy',
                 cv = 10,
                 n_jobs = -1)

gs = gs.fit(X_train, y_train) 

print(gs.best_score_)
print(gs.best_estimator_)

# Using GridSearch to optimize RandomForest
from sklearn.model_selection import GridSearchCV


pipe_rf = Pipeline([('scl', StandardScaler()),
                     ('pca', PCA(n_components = 5)),
                     ('clf', RandomForestClassifier(max_features=1))])


#param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0]

depth_range =[1,3,5,7,9]
estimator_range=[20,40,60,80,100,120,140]
#feature_range=[1,2,3,4,5,6]

param_grid = [
              {'clf__max_depth': depth_range,
               'clf__n_estimators': estimator_range,
               }]

gs2 = GridSearchCV(estimator = pipe_rf,
                 param_grid = param_grid,
                 scoring = 'accuracy',
                 cv = 10,
                 n_jobs = -1)

gs2 = gs2.fit(X_train, y_train) 

print(gs2.best_score_)
print(gs2.best_estimator_)

# Using GridSearch to Optimize Logistic Regression
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC

pipe_lr = Pipeline([('scl', StandardScaler()), 
                     ('pca', PCA(n_components = 5)),
                     ('clf', LogisticRegression())])

param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0]

param_grid = [{'clf__C': param_range}]

gs3 = GridSearchCV(estimator = pipe_lr,
                 param_grid = param_grid,
                 scoring = 'accuracy',
                 cv = 10,
                 n_jobs = -1)

gs3 = gs3.fit(X_train, y_train) 

print(gs3.best_score_)
print(gs3.best_estimator_)

# Using GridSearch to Optimize Neural Network
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC

pipe_mlp = Pipeline([('scl', StandardScaler()), 
                     ('pca', PCA(n_components = 5)),
                     ('clf', MLPClassifier(hidden_layer_sizes=(100,50)))])

param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0]

param_grid = [{'clf__alpha': param_range}]

gs4 = GridSearchCV(estimator = pipe_mlp,
                 param_grid = param_grid,
                 scoring = 'accuracy',
                 cv = 10,
                 n_jobs = -1)

gs4 = gs4.fit(X_train, y_train) 

print(gs4.best_score_)
print(gs4.best_estimator_)

# Using GridSearch to Optimize K Nearst Neighbors
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC

pipe_knn = Pipeline([('scl', StandardScaler()), 
                     ('pca', PCA(n_components = 5)),
                     ('clf', KNeighborsClassifier())])

param_range = [3, 4, 5, 6,7]

param_grid = [{'clf__n_neighbors': param_range}]

gs5 = GridSearchCV(estimator = pipe_knn,
                 param_grid = param_grid,
                 scoring = 'accuracy',
                 cv = 10,
                 n_jobs = -1)

gs5 = gs5.fit(X_train, y_train) 

print(gs5.best_score_)
print(gs5.best_estimator_)

"""## **Try Ensemble Learning to Combine the Result**"""

from sklearn.base import BaseEstimator, clone
from sklearn.base import ClassifierMixin
from sklearn.preprocessing import LabelEncoder
from sklearn.externals import six
from sklearn.pipeline import _name_estimators
import numpy as np
import operator

class MajorityVoteClassifier(BaseEstimator, ClassifierMixin):
    ''' A majority vote ensemble classifier
    Params:
        classifiers : array-like, shape = [n_classifiers]
            Different classifiers fro the ensemble
        
        vote : str, {'classlabel', 'probability'}
            Default 'classlabel'
            If 'classlabel' the prediction is based on 
            the argmax of class labels. Else if 'probability',
            the argmax of the sum of probabilities is used to 
            predict the class label (recommended for calibrated classifiers).
        
        weights : array-like, shape = [n_classifiers]
            Optional, default: None
            If a list of 'int' or 'float' values are provided,
            the classifiers are weighted by importance;
            Uses uniform weights if 'weights=None'.
        '''
    def __init__(self, classifiers,
                vote = 'classlabel', weights = None):
        self.classifiers = classifiers
        self.named_classifiers = {key: value for 
                                 key, value  in _name_estimators(classifiers)}
        self.vote = vote
        self.weights = weights
        
    def fit(self, X, y):
        ''' Fit classifiers.
        Params:
            X : {array-like, sparse matrix}, 
                shape = [n_samples, n_features]
                Matrix of training samples
            y : array-like, shape = [n_samples]
                Vector of target class labels.
        Returns
            self : Object
        '''
        # Use LabelEncoder to ensure class labels start with 0,
        # which is important for np.argmax, call in self.predict
        self.lablenc_ = LabelEncoder()
        self.lablenc_.fit(y)
        self.classes_ = self.lablenc_.classes_
        self.classifiers_ = []
        for clf in self.classifiers:
            fitted_clf = clone(clf).fit(X,
                                       self.lablenc_.transform(y))
            self.classifiers_.append(fitted_clf)
        return self
    
    def predict(self, X):
        ''' Predict class labels for X.
        Params
            X: {array-like, sparse matrix}
                Shape = [n_samples, n_features]
                Matrix of training samples.
        Returns
            maj_vote : array-like, shape = [n_samples]
                Predicted class labels.
        '''
        if self.vote == 'probability':
            maj_vote = np.argmax(self.predict_proba(X), 
                                axis = 1)
        else: # 'classlabel' vote
            # Collect results from clf.predict calls
            predictions = np.asarray([clf.predict(X)
                                     for clf in self.classifiers_]).T
            maj_vote = np.apply_along_axis(
                            lambda x:
                            np.argmax(np.bincount(x, weights=self.weights)),
                                      axis = 1,
                                      arr = predictions)
        maj_vote = self.lablenc_.inverse_transform(maj_vote)
        return maj_vote                            
        
    def predict_proba(self, X):
        ''' Predict class probabilities for X.
        Params:
            X : {array-like, sparse matrix}, 
                shape = [n_samples, n_features]
                Training vectors, where n_samples is 
                the number of samples and n_features is
                the number of features.
        Returns:
        avg_proba : array-like,
            shape = [n_samples, n_classes]
            Weighted average probability for 
            each class per sample.
        '''
        probas = np.asarray([clf.predict_proba(X)
                            for clf in self.classifiers_])
        avg_proba = np.average(probas,
                              axis = 0, weights = self.weights)
        return avg_proba

    def get_params(self, deep=True):
        ''' Get classifier parameters names for GridSearch'''
        if not deep:
            return super(MajorityVoteClassifier,
                        self).get_params(deep=False)
        else:
            out = self.named_classifiers.copy()
            for name, step in six.iteritems(self.named_classifiers):
                for key, value in six.iteritems(
                        step.get_params(deep = True)):
                    out['%s_%s' % (name, key)] = value
            return out

# Using Majority Vote Ensemble Learning,  Need to INCLUDE class MajorityVoteClassifier

X_train, X_test, y_train, y_test = train_test_split(X, y2, 
                            test_size=0.3, 
                            random_state=43)

mv_clf = MajorityVoteClassifier(classifiers = 
                                [
                                  gs.best_estimator_,
                                  gs2.best_estimator_, 
                                  gs3.best_estimator_, 
                                  gs4.best_estimator_,
                                  gs5.best_estimator_])



all_clf = [gs.best_estimator_, 
           gs2.best_estimator_, 
           gs3.best_estimator_, 
           gs4.best_estimator_ ,
           gs5.best_estimator_ ,
           mv_clf]

clf_labels = [
              'Support Vector Machine',
              'Random Forest',
              'Logistic Regression',
              'Neural Network',
              'KNN',
              'Majority Voting']


print("In-Sample Cross-Validation")
for clf, label in zip(all_clf, clf_labels):
    scores = cross_val_score(estimator = clf,
                            X = X_train,
                            y = y_train,
                            cv = 10,
                            scoring = 'accuracy')
    print("Algo: {0:<20s} and Score: {1:0.4f}".format(label, np.mean(scores)))

    
print("Out-of-Sample Cross-Validation")

for clf, label in zip(all_clf, clf_labels):
    clf.fit(X_train,y_train)
    score = clf.score(X_test, y_test)
    print("Algo: {0:<20s} and Score: {1:0.4f}".format(label, score))

"""## **Using Sklearn Regressor to validate the numerical prediction**"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

#import warnings
#warnings.filterwarnings("ignore")

from sklearn.linear_model import Perceptron, LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier


from sklearn.model_selection import cross_val_score

def get_acc(y_test,y_pred,tol):
  num_acc=0
  for i in range(0,len(y_pred)):
    if(y_pred[i] > y_test[i]-tol and y_pred[i] < y_test[i]+tol):
      num_acc+=1
  return num_acc/len(y_pred)
  
X_train, X_test, y_train, y_test = train_test_split(X, y_time, 
                            test_size=0.3, 
                            random_state=0)

Regressor = {
        'Random Forest Regressor': RandomForestRegressor(n_estimators=200),
        'Gradient Boosting Regressor': GradientBoostingRegressor(n_estimators=500),
        'ExtraTrees Regressor': ExtraTreesRegressor(n_estimators=500, min_samples_split=5),
        'Bayesian Ridge': BayesianRidge(),
        'Elastic Net CV': ElasticNetCV()
    }


for name, clf in Regressor.items():
    print(name)
    clf.fit(X_train, y_train)

    print('acc',clf.score(X_test, y_test))  
    #print('new_acc',get_acc(y_test,clf.predict(X_test),10))




    
#         print(f'R2: {r2_score(y_test, clf.predict(X_test)):.2f}')
#         print(f'MAE: {mean_absolute_error(y_test, clf.predict(X_test)):.2f}')
#         print(f'MSE: {mean_squared_error(y_test, clf.predict(X_test)):.2f}')

# Predicting Using RandomForestRegressor
X_train, X_test, y_train, y_test = train_test_split(X, y_time, test_size=0.2, random_state=0)

random_forest=RandomForestRegressor(n_estimators=200).fit(X_train, y_train)
predicted_price = random_forest.predict(X_test) 

predicted_price = pd.DataFrame(predicted_price,index=y_test.index,columns = ['Price'])  
predicted_price.plot(figsize=(20,10))  
y_test.plot()  
plt.legend(['Predicted_price','Actual_price'])   
plt.ylabel("BTC Price")  
plt.show()

# Predicting using Elastic Net CV
X_train, X_test, y_train, y_test = train_test_split(X, y_time, test_size=0.2, random_state=0)

elasticCV=ElasticNetCV().fit(X_train, y_train)
predicted_price = elasticCV.predict(X_test) 

predicted_price = pd.DataFrame(predicted_price,index=y_test.index,columns = ['Price'])  
predicted_price.plot(figsize=(20,10))  
y_test.plot()  
plt.legend(['Predicted_price','Actual_price'])  
plt.ylabel("BTC Price")  
plt.show()

